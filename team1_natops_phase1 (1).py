# -*- coding: utf-8 -*-
"""natops_phase1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_Xc-yaAV3nmdDb_xbQ1GjuCAKJz7VoeS
"""

import os
import pandas as pd
from scipy.io import arff

#TEAM 1: Amanda, Ryan, Avi, Andrew

# Set the path where your ARFF files are extracted
data_folder = "NATOPS"

n_dimensions = 24
train_feature_data = []
test_feature_data = []

# Step 1: Load all dimensions for train and test
for i in range(1, n_dimensions + 1):
    train_path = os.path.join(data_folder, f"NATOPSDimension{i}_TRAIN.arff")
    test_path = os.path.join(data_folder, f"NATOPSDimension{i}_TEST.arff")

    train_data, _ = arff.loadarff(train_path)
    test_data, _ = arff.loadarff(test_path)

    train_df = pd.DataFrame(train_data).drop(columns=["classAttribute"])
    test_df = pd.DataFrame(test_data).drop(columns=["classAttribute"])

    # Rename columns to indicate dimension
    train_df.columns = [f"dim_{i}"] * train_df.shape[1]
    test_df.columns = [f"dim_{i}"] * test_df.shape[1]

    train_feature_data.append(train_df)
    test_feature_data.append(test_df)

# Step 2: Concatenate all 24 dimensions
train_full = pd.concat(train_feature_data, axis=1)
test_full = pd.concat(test_feature_data, axis=1)

# Step 3: Get and clean class labels (decode from bytes if needed)
def decode_class_labels(path):
    raw_labels = pd.DataFrame(arff.loadarff(path)[0])["classAttribute"]
    return raw_labels.apply(lambda x: float(x.decode()) if isinstance(x, bytes) else float(x))

train_labels = decode_class_labels(os.path.join(data_folder, "NATOPS_TRAIN.arff"))
test_labels = decode_class_labels(os.path.join(data_folder, "NATOPS_TEST.arff"))

# Step 4: Add is_test column and start sid from 1
def convert_to_long(df, labels, is_test_flag, offset=0):
    n_samples = df.shape[0]
    n_timesteps = df.shape[1] // n_dimensions
    rows = []

    for sid in range(n_samples):
        for t in range(n_timesteps):
            row = [df.iloc[sid, t + d * n_timesteps] for d in range(n_dimensions)]
            row += [sid + 1 + offset, t, labels[sid], is_test_flag]  # start sid from 1
            rows.append(row)

    return rows

train_rows = convert_to_long(train_full, train_labels, is_test_flag=0)
test_rows = convert_to_long(test_full, test_labels, is_test_flag=1, offset=len(train_labels))

# Step 5: Rename dimension columns to fea1 to fea24
columns = [f"fea{i+1}" for i in range(n_dimensions)] + ["sid", "timestep", "class", "is_test"]
final_df = pd.DataFrame(train_rows + test_rows, columns=columns)
final_df = final_df.drop(columns=["timestep"])

# Save to CSV
final_df.to_csv("natops_processed.csv", index=False)
print("âœ… CSV saved as 'natops_processed.csv' with proper class labels, 'sid', and 'is_test' flags.")

print(final_df.head())


# ============================
# ðŸ“š Works Cited
# ============================

# 1. Ghouaiel, Nehla, Pierre-FranÃ§ois Marteau, and Marc Dupont.
#    "Continuous pattern detection and recognition in stream - a benchmark for online gesture recognition."
#    International Journal of Applied Pattern Recognition, vol. 4, no. 2, 2017, pp. 146â€“160.
#    https://doi.org/10.1504/IJAPR.2017.086618

# 2. McKinney, Wes.
#    "Data Structures for Statistical Computing in Python."
#    Proceedings of the 9th Python in Science Conference, 2010.

# 3. Virtanen, Pauli, et al.
#    "SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python."
#    Nature Methods, vol. 17, 2020, pp. 261â€“272.
#    https://doi.org/10.1038/s41592-019-0686-2

# 4. OpenAI.
#    ChatGPT, April 2025 version.
#    Assistance used for code generation and workflow design.

from google.colab import drive
drive.mount('/content/drive')